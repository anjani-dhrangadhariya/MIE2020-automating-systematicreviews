%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for ma at 2017-08-24 13:06:48 +0200


%% Saved with string encoding Unicode (UTF-8)

% ref for word2vec
@Article{word2vec,
   author = "Tomas Mikolov  and
      Ilya Sutskever  and
      Kai Chen and Greg Corrado and Jeffrey Dean",
   Title="Distributed Representations of Words and Phrases and their Compositionality.",
   Journal="NIPS'13 Proceedings of the 26th International Conference on Neural Information Processing Systems",
   Year="2013",
   Volume="2",
   Pages="3111--3119",
   Month="Dec"
}

% Ref for fastText architecture
@article{bojanowski2017enriching,
  title={Enriching Word Vectors with Subword Information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={5},
  year={2017},
  issn={2307-387X},
  pages={135--146}
}

% 10832224 
@Article{pmid10832224,
   Author="Hunt, D. L.  and Haynes, R. B. ",
   Title="{{H}ow to read a systematic review}",
   Journal="Indian J Pediatr",
   Year="2000",
   Volume="67",
   Number="1",
   Pages="63--66",
   Month="Jan"
}

% 30646959 
@Article{pmid30646959,
   Author="Bannach-Brown, A.  and Przyby?a, P.  and Thomas, J.  and Rice, A. S. C.  and Ananiadou, S.  and Liao, J.  and Macleod, M. R. ",
   Title="{{M}achine learning algorithms for systematic review: reducing workload in a preclinical review of animal studies and reducing human screening error}",
   Journal="Syst Rev",
   Year="2019",
   Volume="8",
   Number="1",
   Pages="23",
   Month="Jan"
}

% Dataset ref for the experiment
@article {Hilfiker651,
	author = {Hilfiker, Roger and Meichtry, Andre and Eicher, Manuela and Nilsson Balfe, Lina and Knols, Ruud H and Verra, Martin L and Taeymans, Jan},
	title = {Exercise and other non-pharmaceutical interventions for cancer-related fatigue in patients during or after cancer treatment: a systematic review incorporating an indirect-comparisons meta-analysis},
	volume = {52},
	number = {10},
	pages = {651--658},
	year = {2018},
	doi = {10.1136/bjsports-2016-096422},
	publisher = {British Association of Sport and Excercise Medicine},
	issn = {0306-3674},
	URL = {https://bjsm.bmj.com/content/52/10/651},
	eprint = {https://bjsm.bmj.com/content/52/10/651.full.pdf},
	journal = {British Journal of Sports Medicine}
}

% sysRevVital
@Article{sysRevVital,
   Author="Anke, R.  and Garner, P. and Young, T.",
   Title="{Reading systematic reviews to answer clinical questions}",
   Journal="Clinical Epidemiology and Global Health",
   Year="2013",
   Volume="2",
   Number="1",
   Pages="39--46",
   Month="Apr",
   DOI="https://doi.org/10.1016/j.cegh.2013.09.001"
}

% 12612111 
@Article{pmid12612111,
   Author="Khan, K. S.  and Kunz, R.  and Kleijnen, J.  and Antes, G. ",
   Title="{{F}ive steps to conducting a systematic review}",
   Journal="J R Soc Med",
   Year="2003",
   Volume="96",
   Number="3",
   Pages="118--121",
   Month="Mar"
}

% Cochrane 
@manual{cochraneHandbook,
    Author = {Higgins, J.P.T. and Green, S.},
    Title = {Cochrane Handbook for Systematic Reviews of Interventions},
    Year = {2011},
    Month = {Mar},
    Language = {English},
    Version = {Version 5.1.0},
    Organization = {The Cochrane Collaboration},
    Pubstate = {forthcoming}
}

% 27884208 
@Article{pmid27884208,
   Author="Haby, M. M.  and Chapman, E.  and Clark, R.  and Barreto, J.  and Reveiz, L.  and Lavis, J. N. ",
   Title="{{W}hat are the best methodologies for rapid reviews of the research evidence for evidence-informed decision making in health policy and practice: a rapid review}",
   Journal="Health Res Policy Syst",
   Year="2016",
   Volume="14",
   Number="1",
   Pages="83",
   Month="Nov"
}

% 22587960 
@Article{pmid22587960,
   Author="Khangura, S.  and Konnyu, K.  and Cushman, R.  and Grimshaw, J.  and Moher, D. ",
   Title="{{E}vidence summaries: the evolution of a rapid review approach}",
   Journal="Syst Rev",
   Year="2012",
   Volume="1",
   Pages="10",
   Month="Feb"
}

% 27515942 
@Article{pmid27515942,
   Author="Bernhardsson, S.  and Lynch, E.  and Dizon, J. M.  and Fernandes, J.  and Gonzalez-Suarez, C.  and Lizarondo, L.  and Luker, J.  and Wiles, L.  and Grimmer, K. ",
   Title="{{A}dvancing {E}vidence-{B}ased {P}ractice in {P}hysical {T}herapy {S}ettings: {M}ultinational {P}erspectives on {I}mplementation {S}trategies and {I}nterventions}",
   Journal="Phys Ther",
   Year="2017",
   Volume="97",
   Number="1",
   Pages="51--60",
   Month="01"
}

% cadima 
@Article{cadima,
   Author="Kohl, C.  and McIntosh, E.J.  and Unger, S.  and Haddaway, N.R.  and Kecke, S.  and Schiemann, J.  and Wilhelm, R.",
   Title="{Online tools supporting the conduct and reporting of systematic reviews and systematic maps: a case study on CADIMA and review of existing tools}",
   Journal="Environmental Evidence",
   Year="2018",
   Volume="7",
   Number="1",
   Pages="12",
   Month="Marc"
}

@Article{Thabane2008,
author="Thabane, Lehana
and Thomas, Tara
and Ye, Chenglin
and Paul, James",
title="Posing the research question: not so simple",
journal="Canadian Journal of Anesthesia/Journal canadien d'anesth{\'e}sie",
year="2008",
month="Dec",
day="24",
volume="56",
number="1",
pages="71",
issn="1496-8975",
doi="10.1007/s12630-008-9007-4",
url="https://doi.org/10.1007/s12630-008-9007-4"
}

% Class imbalance and class overlap combined
@INPROCEEDINGS{Garcia2006-wn,
  title     = "Combined Effects of Class Imbalance and Class Overlap on
               {Instance-Based} Classification",
  booktitle = "Intelligent Data Engineering and Automated Learning -- {IDEAL}
               2006",
  author    = "Garc{\'\i}a, V and Alejo, R and S{\'a}nchez, J S and Sotoca, J M
               and Mollineda, R A",
  abstract  = "In real-world applications, it has been often observed that
               class imbalance (significant differences in class prior
               probabilities) may produce an important deterioration of the
               classifier performance, in particular with patterns belonging to
               the less represented classes. This effect becomes especially
               significant on instance-based learning due to the use of some
               dissimilarity measure. We analyze the effects of class imbalance
               on the classifier performance and how the overlap has influence
               on such an effect, as well as on several techniques proposed in
               the literature to tackle the class imbalance. Besides, we study
               how these methods affect to the performance on both classes, not
               only on the minority class as usual.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "371--378",
  year      =  2006
}

% how to choose parameters for CNN text classification?
@ARTICLE{Zhang2015-qd,
  title         = "A Sensitivity Analysis of (and Practitioners' Guide to)
                   Convolutional Neural Networks for Sentence Classification",
  author        = "Zhang, Ye and Wallace, Byron",
  abstract      = "Convolutional Neural Networks (CNNs) have recently achieved
                   remarkably strong performance on the practically important
                   task of sentence classification (kim 2014, kalchbrenner
                   2014, johnson 2014). However, these models require
                   practitioners to specify an exact model architecture and set
                   accompanying hyperparameters, including the filter region
                   size, regularization parameters, and so on. It is currently
                   unknown how sensitive model performance is to changes in
                   these configurations for the task of sentence
                   classification. We thus conduct a sensitivity analysis of
                   one-layer CNNs to explore the effect of architecture
                   components on model performance; our aim is to distinguish
                   between important and comparatively inconsequential design
                   decisions for sentence classification. We focus on one-layer
                   CNNs (to the exclusion of more complex models) due to their
                   comparative simplicity and strong empirical performance,
                   which makes it a modern standard baseline method akin to
                   Support Vector Machine (SVMs) and logistic regression. We
                   derive practical advice from our extensive empirical results
                   for those interested in getting the most out of CNNs for
                   sentence classification in real world settings.",
  month         =  oct,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1510.03820"
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Chawla2002-gn,
  title     = "{SMOTE}: synthetic minority over-sampling technique",
  author    = "Chawla, N V and Bowyer, K W and Hall, L O and {others}",
  abstract  = "An approach to the construction of classifiers from imbalanced
               datasets is described. A dataset is imbalanced if the
               classification categories are not approximately equally
               represented. Often real-world data sets are predominately
               composed of``normal''examples with only a small percentage
               of``abnormal''or``interesting''examples. It is also the case
               that the cost of misclassifying an abnormal (interesting)
               example as a normal example is often much higher than the cost
               of the reverse error. Under-sampling of the majority (normal)
               class â€¦",
  journal   = "J. Artif. Organs",
  publisher = "jair.org",
  year      =  2002
}

% word2vec and fastText
@ARTICLE{Goldberg2015-qv,
  title         = "A Primer on Neural Network Models for Natural Language
                   Processing",
  author        = "Goldberg, Yoav",
  abstract      = "Over the past few years, neural networks have re-emerged as
                   powerful machine-learning models, yielding state-of-the-art
                   results in fields such as image recognition and speech
                   processing. More recently, neural network models started to
                   be applied also to textual natural language signals, again
                   with very promising results. This tutorial surveys neural
                   network models from the perspective of natural language
                   processing research, in an attempt to bring natural-language
                   researchers up to speed with the neural techniques. The
                   tutorial covers input encoding for natural language tasks,
                   feed-forward networks, convolutional networks, recurrent
                   networks and recursive networks, as well as the computation
                   graph abstraction for automatic gradient computation.",
  month         =  oct,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1510.00726"
}

% 25738806 PR-AUC 
@Article{pmid25738806,
   Author="Saito, T.  and Rehmsmeier, M. ",
   Title="{{T}he precision-recall plot is more informative than the {R}{O}{C} plot when evaluating binary classifiers on imbalanced datasets}",
   Journal="PLoS ONE",
   Year="2015",
   Volume="10",
   Number="3",
   Pages="e0118432"
}

% Automation - Citation Screening
@ARTICLE{Lerner2019-tj,
  title    = "Automatic screening using word embeddings achieved high
              sensitivity and workload reduction for updating living network
              meta-analyses",
  author   = "Lerner, Ivan and Cr{\'e}quit, Perrine and Ravaud, Philippe and
              Atal, Ignacio",
  abstract = "OBJECTIVES: We aimed to develop and evaluate an algorithm for
              automatically screening citations when updating living network
              meta-analysis (NMA). STUDY DESIGN AND SETTING: Our algorithm
              learns from the initial screening of citations conducted when
              creating an NMA to automatically identify eligible citations
              (i.e., needing full-text consideration) when updating the NMA. We
              evaluated our algorithm on four NMAs from different medical
              domains. For each NMA we constructed sets of initially screened
              citations and citations to screen during an update that took
              place 2 years after the conduct of the NMA. We encoded free text
              of citations (title and abstract) using word embeddings. On top
              of this vectorized representation, we fitted a logistic
              regression model to the set of initially screened citations to
              predict the eligibility of citations screened during an update.
              RESULTS: Our algorithm achieved 100\% sensitivity on two NMAs
              (100\% [95\% confidence interval 93-100] and 100\% [40-100]
              sensitivity), and 94\% (81-99) and 97\% (86-100) on the remaining
              two others. For all NMAs, our algorithm would have spared to
              manually screen 1,345 of 2,530 citations, decreasing the workload
              by 53\% (51-55), while missing 3 of 124 eligible citations (2\%
              [1-7]), none of which were finally included in the NMAs after
              full-text consideration. CONCLUSION: For updating an NMA after 2
              years, our algorithm considerably diminished the workload
              required for screening, and the number of missed eligible
              citations remained low.",
  journal  = "J. Clin. Epidemiol.",
  volume   =  108,
  pages    = "86--94",
  month    =  apr,
  year     =  2019,
  keywords = "Automatic screening; Live cumulative network meta-analysis;
              Machine learning; Natural language processing; Network
              meta-analysis; Word embeddings",
  language = "en"
}
